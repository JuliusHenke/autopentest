import os
from typing import List, Tuple

from dotenv import load_dotenv
from langchain.agents import AgentExecutor
from langchain.agents.format_scratchpad import format_to_openai_function_messages
from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    format_document,
)
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.prompts.prompt import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import (
    RunnableParallel,
)
from langchain_core.utils.function_calling import format_tool_to_openai_function

from agents.task_specific_agents import vulnerable_and_outdated_components_agent, TaskSpecificAgent
from chat_completions import llm
from text_embeddings import retriever

# Load environment variables
load_dotenv()
pinecone_index = os.environ.get('PINECONE_INDEX', 'autopentest')

# Conversational Retrieval Chain
default_document_prompt = PromptTemplate.from_template(template="{page_content}")


def _combine_documents(
        docs, document_prompt=default_document_prompt, document_separator="\n\n"
):
    doc_strings = [format_document(doc, document_prompt) for doc in docs]
    return document_separator.join(doc_strings)


# User input
class RagChainInputType(BaseModel):
    messages: List[Tuple[str, str]] = Field(..., extra={"widget": {"type": "chat"}})
    question: str


def rag_chain(agent: TaskSpecificAgent):
    # Condense a chat history and follow-up question into a standalone question
    condense_question_prompt_template = """Given the following messages, create a standalone question summarizing what to do next.
    Messages:
    {messages}
    Standalone question:"""
    condense_question_prompt = PromptTemplate.from_template(condense_question_prompt_template)
    _search_query = condense_question_prompt | llm | StrOutputParser()

    rag_chain_inputs = RunnableParallel(
        {
            "messages": lambda x: x["messages"],
            "context": _search_query | retriever(agent.vector_db_namespace) | _combine_documents,
            "agent_scratchpad": lambda x: format_to_openai_function_messages(
                x["intermediate_steps"]
            ),
        }
    ).with_types(input_type=RagChainInputType)
    llm_with_tools = llm.bind(
        functions=[format_tool_to_openai_function(t) for t in agent.tools])

    answer_prompt_template = (agent.system_prompt
                              + "\nDo your work based on the following context:"
                              + "\n<context>\n{context}\n</context>"
                              )
    rag_chain_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", answer_prompt_template),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )

    return rag_chain_inputs | rag_chain_answer_prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()


# Usage example
if __name__ == '__main__':
    executor = AgentExecutor(
        agent=rag_chain(vulnerable_and_outdated_components_agent),
        tools=vulnerable_and_outdated_components_agent.tools,
        verbose=True,
    ).with_types(input_type=RagChainInputType)
    executor.invoke({
        "messages": [
            HumanMessage(content="What is the best fruit?"),
            AIMessage(content="Apple"),
            HumanMessage(content="Is there a company named after this fruit?"),
        ],
    })
